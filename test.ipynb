{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nlu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AllComponentsInfo',\n",
       " 'Callable',\n",
       " 'ComponentUniverse',\n",
       " 'ComponentUtils',\n",
       " 'Dict',\n",
       " 'Discoverer',\n",
       " 'FeatureResolutions',\n",
       " 'LicenseType',\n",
       " 'Licenses',\n",
       " 'LightPipeline',\n",
       " 'List',\n",
       " 'ModelBuckets',\n",
       " 'NLP_FEATURES',\n",
       " 'NLP_HC_FEATURES',\n",
       " 'NLUPipeline',\n",
       " 'NluComponent',\n",
       " 'OCR_FEATURES',\n",
       " 'Optional',\n",
       " 'PipeUtils',\n",
       " 'Pipeline',\n",
       " 'PipelineModel',\n",
       " 'PipelineQueryVerifier',\n",
       " 'PretrainedPipeline',\n",
       " 'Spellbook',\n",
       " 'Union',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'anno_class_to_empty_component',\n",
       " 'auth',\n",
       " 'auth_utils',\n",
       " 'authenticate_enviroment_HC',\n",
       " 'authenticate_enviroment_HC_and_OCR',\n",
       " 'authenticate_enviroment_OCR',\n",
       " 'autocomplete_pipeline',\n",
       " 'ch',\n",
       " 'check_if_nlu_ref_is_licensed',\n",
       " 'check_if_secret_missmatch_and_uninstall_if_bad',\n",
       " 'check_pyspark_install',\n",
       " 'components',\n",
       " 'disable_verbose',\n",
       " 'discoverer',\n",
       " 'discovery',\n",
       " 'enable_streamlit_caching',\n",
       " 'enable_verbose',\n",
       " 'env_utils',\n",
       " 'get_authenticated_spark',\n",
       " 'get_authenticated_spark_HC',\n",
       " 'get_authenticated_spark_HC_and_OCR',\n",
       " 'get_authenticated_spark_OCR',\n",
       " 'get_component_list_for_iterable_stages',\n",
       " 'get_components',\n",
       " 'get_nlu_pipe_for_nlp_pipe',\n",
       " 'get_open_source_spark_context',\n",
       " 'get_pyspark_major_and_minor',\n",
       " 'get_pyspark_version',\n",
       " 'get_trainable_component_for_nlu_ref',\n",
       " 'get_trained_component_for_nlp_model_ref',\n",
       " 'get_trained_component_list_for_nlp_pipe_ref',\n",
       " 'github_issues_link',\n",
       " 'healthcare_version_is_missmatch',\n",
       " 'import_or_install_licensed_lib',\n",
       " 'importlib',\n",
       " 'info',\n",
       " 'init_component',\n",
       " 'install_and_import_package',\n",
       " 'is_authorized_environment',\n",
       " 'is_env_pyspark_2_3',\n",
       " 'is_env_pyspark_2_4',\n",
       " 'is_env_pyspark_2_x',\n",
       " 'is_env_pyspark_3_0',\n",
       " 'is_env_pyspark_3_1',\n",
       " 'is_env_pyspark_3_2',\n",
       " 'is_env_pyspark_3_x',\n",
       " 'is_produced_by_multi_output_component',\n",
       " 'is_running_in_databricks',\n",
       " 'json',\n",
       " 'license_to_bucket',\n",
       " 'load',\n",
       " 'load_nlu_pipe_from_hdd',\n",
       " 'logger',\n",
       " 'logging',\n",
       " 'nlu',\n",
       " 'nlu_package_location',\n",
       " 'nlu_ref_to_component',\n",
       " 'nlu_ref_to_nlp_metadata',\n",
       " 'nlu_ref_to_nlp_ref',\n",
       " 'ocr_components',\n",
       " 'ocr_version_is_missmatch',\n",
       " 'offline_utils',\n",
       " 'os',\n",
       " 'parse_language_from_nlu_ref',\n",
       " 'pipe',\n",
       " 'print_all_languages',\n",
       " 'print_all_model_kinds_for_action',\n",
       " 'print_all_model_kinds_for_action_and_lang',\n",
       " 'print_all_nlu_components_for_lang',\n",
       " 'print_component_types',\n",
       " 'print_components',\n",
       " 'print_trainable_components',\n",
       " 'reload',\n",
       " 'resolve_feature',\n",
       " 'resolve_storage_ref',\n",
       " 'set_cols_on_nlu_components',\n",
       " 'set_storage_ref_and_resolution_on_component_info',\n",
       " 'site',\n",
       " 'slack_link',\n",
       " 'sparknlp',\n",
       " 'spellbook',\n",
       " 'st_cache_enabled',\n",
       " 'sys',\n",
       " 'to_nlu_pipe',\n",
       " 'to_pretty_df',\n",
       " 'try_import_pyspark_in_streamlit',\n",
       " 'try_import_spark_nlp',\n",
       " 'try_import_streamlit',\n",
       " 'uninstall_lib',\n",
       " 'universe',\n",
       " 'version',\n",
       " 'viz',\n",
       " 'warnings',\n",
       " 'wrap_with_st_cache_if_available_and_set_layout_to_wide']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nlu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m em \u001b[39m=\u001b[39m nlu\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mbiobert\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\alsrb\\anaconda3\\envs\\ai\\lib\\site-packages\\nlu\\__init__.py:199\u001b[0m, in \u001b[0;36mload\u001b[1;34m(request, path, verbose, gpu, streamlit_caching, m1_chip)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[39m# check if secrets are in default loc, if yes load them and create licensed context automatically\u001b[39;00m\n\u001b[0;32m    198\u001b[0m auth(gpu\u001b[39m=\u001b[39mgpu)\n\u001b[1;32m--> 199\u001b[0m spark \u001b[39m=\u001b[39m get_open_source_spark_context(gpu, m1_chip)\n\u001b[0;32m    200\u001b[0m spark\u001b[39m.\u001b[39mcatalog\u001b[39m.\u001b[39mclearCache()\n\u001b[0;32m    202\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n",
      "File \u001b[1;32mc:\\Users\\alsrb\\anaconda3\\envs\\ai\\lib\\site-packages\\nlu\\__init__.py:370\u001b[0m, in \u001b[0;36mget_open_source_spark_context\u001b[1;34m(gpu, m1_chip)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[39mreturn\u001b[39;00m sparknlp\u001b[39m.\u001b[39mstart(gpu\u001b[39m=\u001b[39mgpu, m1\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    369\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 370\u001b[0m         \u001b[39mreturn\u001b[39;00m sparknlp\u001b[39m.\u001b[39;49mstart(gpu\u001b[39m=\u001b[39;49mgpu)\n\u001b[0;32m    371\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailure starting Spark Context! Current Spark version \u001b[39m\u001b[39m{\u001b[39;00mget_pyspark_version()\u001b[39m}\u001b[39;00m\u001b[39m not supported! \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m                  \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPlease install any of Pyspark 3.X versions.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alsrb\\anaconda3\\envs\\ai\\lib\\site-packages\\sparknlp\\__init__.py:244\u001b[0m, in \u001b[0;36mstart\u001b[1;34m(gpu, m1, memory, cache_folder, log_folder, cluster_tmp_dir, real_time_output, output_level)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkRealTimeOutput()\n\u001b[0;32m    243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 244\u001b[0m     spark_session \u001b[39m=\u001b[39m start_without_realtime_output()\n\u001b[0;32m    245\u001b[0m     \u001b[39mreturn\u001b[39;00m spark_session\n",
      "File \u001b[1;32mc:\\Users\\alsrb\\anaconda3\\envs\\ai\\lib\\site-packages\\sparknlp\\__init__.py:154\u001b[0m, in \u001b[0;36mstart.<locals>.start_without_realtime_output\u001b[1;34m()\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mif\u001b[39;00m cluster_tmp_dir \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    152\u001b[0m     builder\u001b[39m.\u001b[39mconfig(\u001b[39m\"\u001b[39m\u001b[39mspark.jsl.settings.storage.cluster_tmp_dir\u001b[39m\u001b[39m\"\u001b[39m, cluster_tmp_dir)\n\u001b[1;32m--> 154\u001b[0m \u001b[39mreturn\u001b[39;00m builder\u001b[39m.\u001b[39;49mgetOrCreate()\n",
      "File \u001b[1;32mc:\\Users\\alsrb\\anaconda3\\envs\\ai\\lib\\site-packages\\pyspark\\sql\\session.py:186\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m         sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[0;32m    185\u001b[0m     \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[0;32m    187\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m    189\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[1;32mc:\\Users\\alsrb\\anaconda3\\envs\\ai\\lib\\site-packages\\pyspark\\context.py:376\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    375\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 376\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[0;32m    377\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\alsrb\\anaconda3\\envs\\ai\\lib\\site-packages\\pyspark\\context.py:133\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    130\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    131\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 133\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    134\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m    136\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[1;32mc:\\Users\\alsrb\\anaconda3\\envs\\ai\\lib\\site-packages\\pyspark\\context.py:325\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    324\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[1;32m--> 325\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    326\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[0;32m    328\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\alsrb\\anaconda3\\envs\\ai\\lib\\site-packages\\pyspark\\java_gateway.py:105\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.1\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 105\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mJava gateway process exited before sending its port number\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(conn_info_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m info:\n\u001b[0;32m    108\u001b[0m     gateway_port \u001b[39m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "em = nlu.load('biobert')\n",
    "\n",
    "# 파일들 내려받고 경로설정도 다 했는데..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84bde07b03191aeadc15fbdbc5a9129eed61ddbc860a10d60e3c5f1f59b9c754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
